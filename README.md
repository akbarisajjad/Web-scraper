# Web-scraper-plus
# اسکرپر وب سایت با استفاده از Selenium و BeautifulSoup

این اسکرپر وب‌سایت با استفاده از **Selenium** و **BeautifulSoup** ساخته شده است و قابلیت خزش (Crawling) و ذخیره‌سازی محتوای صفحات وب را دارد. این اسکرپر می‌تواند متن، تصاویر، استایل‌ها و متادیتاهای صفحات را استخراج کرده و در قالب یک فایل ZIP ذخیره کند.

## ویژگی‌ها

- **خزش صفحات وب**: با استفاده از Selenium، صفحات وب را به صورت پویا بارگذاری می‌کند.
- **استخراج متن**: متن اصلی صفحات را استخراج و ذخیره می‌کند.
- **ذخیره‌سازی تصاویر**: تصاویر موجود در صفحات را دانلود و ذخیره می‌کند.
- **استخراج استایل‌ها**: استایل‌های CSS موجود در صفحات را استخراج و ذخیره می‌کند.
- **ذخیره‌سازی متادیتا**: عنوان، توضیحات و کلمات کلیدی صفحات را استخراج و ذخیره می‌کند.
- **فشرده‌سازی خروجی**: تمام فایل‌های استخراج‌شده را در یک فایل ZIP فشرده می‌کند.

## نیازمندی‌ها

برای اجرای این اسکرپر، نیاز به نصب کتابخانه‌های زیر دارید:

- **Python 3.7 یا بالاتر**
- **Selenium**
- **BeautifulSoup4**
- **Requests**
- **tqdm**
- **Zipfile**

برای نصب کتابخانه‌ها، دستور زیر را اجرا کنید:
bash

```
pip install selenium beautifulsoup4 requests tqdm
```

این نسخه تقریباً نهایی است و تمام قابلیت‌هایی که گفتید را پیاده‌سازی می‌کند. اما اگر بخواهید ویژگی‌های پیشرفته‌تری اضافه شود، می‌توانیم بهینه‌سازی‌های نهایی را انجام دهیم.

📌 مواردی که می‌توان بهبود داد:

1. بهینه‌سازی دریافت تصاویر و فایل‌ها:

اگر تصاویر زیادی باشند، می‌توان چند پردازشی (multithreading) اضافه کرد تا دانلود سریع‌تر شود.



2. فشرده‌سازی خروجی:

قابلیت ذخیره تمام داده‌های صفحه در یک فایل ZIP.



3. ذخیره اطلاعات متا (عنوان، توضیحات، کلمات کلیدی)

metadata.txt شامل:

<title>

<meta name="description">

<meta name="keywords">




4. امکان استخراج فقط از یک بخش خاص (مثلاً فقط مقاله، فقط نظرات و...):

قابلیت انتخاب بخش‌های خاص (مثلاً فقط <article> را استخراج کند).



5. پشتیبانی از سایت‌های داینامیک (React, Vue, AJAX)

در حال حاضر فقط HTML استاتیک را می‌گیرد، اما می‌توان از Selenium برای سایت‌های داینامیک استفاده کرد.



6. امکان تعیین عمق خزش (Crawl Depth)

مشخص کنید تا چند سطح از لینک‌های داخلی بررسی شود.





---

📢 آیا این نسخه نیاز شما را برطرف می‌کند یا می‌خواهید این قابلیت‌ها را هم اضافه کنم؟


خیلی هم عالی! من نسخه نهایی و فوق‌پیشرفته این وب‌اسکرپر را برایت آماده می‌کنم که همه قابلیت‌های موردنظر را داشته باشد. ویژگی‌های جدید شامل:

-- چندپردازشی برای دانلود سریع‌تر تصاویر و فایل‌ها
-- فشرده‌سازی تمام داده‌های صفحه در یک فایل ZIP
-- استخراج اطلاعات متا (title, description, keywords)
-- قابلیت استخراج فقط از یک بخش خاص (مثلاً فقط مقاله، نظرات و...)
-- پشتیبانی از سایت‌های داینامیک (React, Vue, AJAX) با Selenium
-- امکان تنظیم عمق خزش (Crawl Depth)

کد را در حال آماده‌سازی دارم و به‌زودی نسخه کامل را ارسال می‌کنم! 🚀


برای اجرای این وب اسکرپر، باید کتابخانه‌های زیر را نصب کنید:

📌 کتابخانه‌های مورد نیاز:
برای نمایش متن `console.log("Hello, World!");` درون خط، از بک‌تیک استفاده کنید.
```
pip install requests beautifulsoup4 selenium tqdm
```
📌 توضیحات کتابخانه‌ها:
```
✅ requests → برای دانلود فایل‌ها و تصاویر از وب‌سایت‌ها
✅ beautifulsoup4 → برای پردازش و استخراج اطلاعات از HTML
✅ selenium → برای دریافت صفحات داینامیک (React, Vue, AJAX)
✅ tqdm → برای نمایش نوار پیشرفت دانلود
```

---

📌 نصب در ویندوز یا لینوکس
```
pip install -r requirements.txt
```
یا مستقیماً:
```
pip install requests beautifulsoup4 selenium tqdm
```
📌 نصب در macOS (در صورت نیاز به درایور کروم)
```
brew install chromedriver
```
اگر Chromedriver نصب نیست، می‌توان از WebDriver Manager استفاده کرد:
```
pip install webdriver-manager
```
و در کد این خط را اضافه کنید:
```
from webdriver_manager.chrome import ChromeDriverManager
driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)
```
🔍 معرفی وب‌اسکرپر پیشرفته

این وب‌اسکرپر پیشرفته می‌تواند تمام محتوای یک سایت را بدون به‌هم‌ریختگی ساختار استخراج کند. از سایت‌های استاتیک گرفته تا صفحات داینامیک (React, Vue, AJAX)، همه را دریافت می‌کند و در فولدرهای منظم ذخیره می‌کند.

📌 قابلیت‌های کلیدی

1️⃣ دانلود کامل صفحات سایت

✅ دریافت کامل HTML صفحه و ذخیره به همان شکل اصلی در فایل index.html

✅ استخراج و ذخیره متن صفحه بدون کدهای اضافی در فایل content.txt

✅ ذخیره متادیتا (عنوان، توضیحات و کلمات کلیدی) در metadata.txt


---

2️⃣ استخراج و ذخیره استایل‌ها (CSS) به‌صورت جداگانه

- فایل global.css شامل تمام استایل‌های صفحه (تگ <style>)
- 
- دانلود فایل‌های CSS خارجی (.css) و ذخیره در فولدر styles/
- 
- استایل هر بخش مانند هدر، فوتر، محتوا و... به‌صورت جداگانه حفظ می‌شود


---

3️⃣ دانلود و ذخیره تصاویر و فایل‌ها

- تمام تصاویر (.jpg, .png, .svg, .gif) به فولدر images/ دانلود می‌شود
- 
- پشتیبانی از فایل‌های ویدیویی (.mp4, .webm) و ذخیره در videos/
- 
- چندپردازشی (Multi-threading) برای افزایش سرعت دانلود تصاویر


---

4️⃣ پشتیبانی از سایت‌های داینامیک (React, Vue, AJAX)

- استفاده از Selenium برای دریافت صفحات JavaScript-based
- 
- استخراج محتوای سایت‌هایی که محتوا را به‌صورت AJAX لود می‌کنند
- 
- اجرای خودکار مرورگر کروم بدون نیاز به باز کردن آن (Headless Mode)


---

5️⃣ استخراج از بخش‌های خاص صفحه (قابلیت انتخابی)

✅ می‌توانید مشخص کنید که فقط بخش‌های خاصی از سایت را استخراج کند

✅ مثال: فقط <article> (مقاله‌ها)، <comments> (نظرات کاربران) یا <main>


---

6️⃣ تنظیم عمق خزش (Crawl Depth)

✅ امکان مشخص کردن تا چند سطح از لینک‌های داخلی پردازش شود

✅ مثال:

CRAWL_DEPTH = 0 → فقط صفحه اصلی

CRAWL_DEPTH = 1 → صفحه اصلی + لینک‌های داخلی آن

CRAWL_DEPTH = 2 → صفحه اصلی + لینک‌های داخلی + لینک‌های داخلی آن‌ها



---

7️⃣ فشرده‌سازی خروجی (ZIP)

✅ تمام فایل‌های استخراج‌شده در یک فایل ZIP ذخیره می‌شوند

✅ امکان ارسال یا انتقال سریع به سیستم‌های دیگر


---

📂 ساختار خروجی
```
📂 scraped_site/ (هر صفحه در یک فولدر مجزا ذخیره می‌شود)

scraped_site/
│── index.html       (کد کامل HTML صفحه)
│── content.txt      (متن خالص بدون کدهای اضافی)
│── metadata.txt     (عنوان، توضیحات و کلمات کلیدی)
│── global.css       (استایل‌های داخلی صفحه)
│── images/
│   ├── image1.jpg
│   ├── image2.png
│── styles/
│   ├── main.css
│   ├── theme.css
│── scripts/
│   ├── script.js
│── videos/
│   ├── video.mp4
scraped_site.zip     (کل داده‌ها در یک فایل ZIP)
```

---

🔧 کتابخانه‌های موردنیاز و نحوه اجرا


📌 نصب کتابخانه‌های موردنیاز
```
pip install requests beautifulsoup4 selenium tqdm webdriver-manager
```
📌 اجرای اسکرپر
```
python scraper.py
```
(آدرس سایت را در متغیر BASE_URL تنظیم کنید)


---

⚡ چرا این وب‌اسکرپر عالی است؟

✅ پشتیبانی از سایت‌های معمولی + سایت‌های داینامیک

✅ دانلود سریع تصاویر و فایل‌ها با چندپردازشی

✅ ساختار خروجی کاملاً منظم و دسته‌بندی‌شده

✅ قابلیت فشرده‌سازی داده‌ها در فایل ZIP

✅ استخراج استایل‌ها، متن‌ها و متادیتا به‌صورت جداگانه

✅ بهینه‌شده برای انواع سایت‌ها و مقیاس‌های مختلف


---

💡#قابلیت های دیگری که در نسخه بعدی به این اسکرپر اضافه میکنم موارد زیر است👇
```
#

```
🚀 ##قابلیت‌های پیشرفته‌ای که می‌توان به اسکرپر اضافه کرد

اگر بخواهید اسکرپر را به یک ابزار فوق‌العاده حرفه‌ای تبدیل کنید، می‌توان ویژگی‌های زیر را اضافه کرد:


---

1️⃣ ##پشتیبانی از ورود به سایت‌ها (Login)

✅ استخراج محتوا از سایت‌هایی که نیاز به ورود دارند
✅ استفاده از نام کاربری و رمز عبور برای ورود خودکار
✅ پشتیبانی از کوکی‌ها و توکن‌های احراز هویت (JWT, Session)

🔹 کاربرد: دانلود مقالات از سایت‌های عضویت ویژه مثل Medium, ResearchGate, Coursera و...


---

2️⃣ #دریافت خودکار تمام صفحات یک سایت (Site Crawler)

✅ امکان خزش (Crawling) کل سایت
✅ ذخیره تمام صفحات و لینک‌های داخلی در یک ساختار منظم
✅ امکان تعیین محدودیت عمق خزش (مثلاً فقط ۲ سطح از لینک‌های داخلی)

🔹 کاربرد: استخراج کل محتوای سایت‌ها برای تحلیل محتوا، سئو و آرشیو کردن


---

3️⃣ شبیه‌سازی رفتار کاربر (Anti-Bot Bypass)

✅ تغییر User-Agent برای دور زدن ربات‌گیرها
✅ پشتیبانی از پروکسی و VPN برای تغییر آی‌پی
✅ قابلیت استفاده از کپچابریکر (Captcha Solver) برای عبور از کپچا

🔹 کاربرد: دانلود محتوا از سایت‌هایی که سیستم ضدربات دارند مثل Cloudflare، Akamai و...


---

4️⃣ استخراج اطلاعات ساختاریافته (Schema & JSON-LD)

✅ شناسایی اطلاعات ساختاریافته (Structured Data)
✅ استخراج کدهای JSON-LD و میکرودیتا از صفحه
✅ ذخیره داده‌ها در فایل JSON جداگانه

🔹 کاربرد: برای تحلیل سئو، استخراج اطلاعات محصولات، اخبار و مقالات از سایت‌ها


---

5️⃣ پردازش و خلاصه‌سازی محتوای متنی

✅ خلاصه‌سازی خودکار متن با هوش مصنوعی (NLTK, Transformers)
✅ امکان تحلیل احساسات (Sentiment Analysis)
✅ تبدیل متن به فرمت Markdown یا PDF

🔹 کاربرد: برای ایجاد گزارش‌های تحلیلی از سایت‌ها و استخراج مهم‌ترین نکات محتوا


---

6️⃣ دانلود فایل‌های پیوست (PDF, DOCX, ZIP)

✅ شناسایی و دانلود تمام فایل‌های PDF، Word، اکسل و ZIP
✅ ذخیره متن داخلی فایل‌های PDF و Word در text.txt
✅ تبدیل فایل‌های PDF به قالب متنی قابل ویرایش

🔹 کاربرد: دانلود گزارش‌ها، مقالات دانشگاهی و فایل‌های منابع


---

7️⃣ ذخیره‌سازی داده‌ها در دیتابیس

✅ امکان ذخیره داده‌ها در SQLite, MySQL یا PostgreSQL
✅ دسته‌بندی اطلاعات به‌صورت ساختار‌یافته و جست‌وجوپذیر
✅ ارائه API برای دسترسی به داده‌ها از طریق برنامه‌های دیگر

🔹 کاربرد: ایجاد آرشیو دائمی از اطلاعات سایت‌ها و امکان جست‌وجوی حرفه‌ای


---

8️⃣ پشتیبانی از سایت‌های چندزبانه

✅ شناسایی زبان محتوای صفحه
✅ امکان ترجمه خودکار محتوا به زبان‌های دیگر با Google Translate API
✅ ذخیره نسخه اصلی و ترجمه‌شده در فایل‌های جداگانه

🔹 کاربرد: اسکرپ کردن مقالات چندزبانه و سایت‌های بین‌المللی


---

9️⃣ دانلود کامنت‌ها و نظرات کاربران

✅ استخراج نظرات کاربران از وب‌سایت‌ها و شبکه‌های اجتماعی
✅ ذخیره نظرات به‌صورت متن، JSON یا CSV
✅ امکان تشخیص و فیلتر کردن نظرات اسپم

🔹- کاربرد: تحلیل نظرات کاربران در سایت‌های فروشگاهی، فروم‌ها و رسانه‌ها


---

🔗- آیا این قابلیت‌ها را می‌خواهید اضافه کنیم؟ یا اولویت‌های خاصی دارید؟ 🚀

🚀- قابلیت‌های پیشرفته‌ای که می‌توان به اسکرپر اضافه کرد


✅- تبدیل متن به فرمت Markdown یا PDF

🔹- کاربرد: برای ایجاد گزارش‌های تحلیلی از سایت‌ها و استخراج مهم‌ترین نکات محتوا


---

6️⃣- دانلود فایل‌های پیوست (PDF, DOCX, ZIP)

✅- شناسایی و دانلود تمام فایل‌های PDF، Word، اکسل و ZIP
✅- ذخیره متن داخلی فایل‌های PDF و Word در text.txt
✅- تبدیل فایل‌های PDF به قالب متنی قابل ویرایش

🔹- کاربرد: دانلود گزارش‌ها، مقالات دانشگاهی و فایل‌های منابع


---

7️⃣- ذخیره‌سازی داده‌ها در دیتابیس

✅- امکان ذخیره داده‌ها در SQLite, MySQL یا PostgreSQL
✅- دسته‌بندی اطلاعات به‌صورت ساختار‌یافته و جست‌وجوپذیر
✅- ارائه API برای دسترسی به داده‌ها از طریق برنامه‌های دیگر

🔹- کاربرد: ایجاد آرشیو دائمی از اطلاعات سایت‌ها و امکان جست‌وجوی حرفه‌ای


---

8️⃣- پشتیبانی از سایت‌های چندزبانه

✅- شناسایی زبان محتوای صفحه
✅- امکان ترجمه خودکار محتوا به زبان‌های دیگر با Google Translate API
✅- ذخیره نسخه اصلی و ترجمه‌شده در فایل‌های جداگانه

🔹 کاربرد: اسکرپ کردن مقالات چندزبانه و سایت‌های بین‌المللی


---

9️⃣ دانلود کامنت‌ها و نظرات کاربران

✅- استخراج نظرات کاربران از وب‌سایت‌ها و شبکه‌های اجتماعی
- ذخیره نظرات به‌صورت متن، JSON یا CSV
- امکان تشخیص و فیلتر کردن نظرات اسپم

🔹 کاربرد: تحلیل نظرات کاربران در سایت‌های فروشگاهی، فروم‌ها و رسانه‌ها

قطعاً! در اینجا یک فایل `README.md` حرفه‌ای برای اسکرپر گوگل کروم شما نوشته شده است. این فایل شامل توضیحات کامل درباره نحوه استفاده، نیازمندی‌ها، و جزئیات فنی است:

---

```markdown
# اسکرپر وب سایت با استفاده از Selenium و BeautifulSoup

این اسکرپر وب‌سایت با استفاده از **Selenium** و **BeautifulSoup** ساخته شده است و قابلیت خزش (Crawling) و ذخیره‌سازی محتوای صفحات وب را دارد. این اسکرپر می‌تواند متن، تصاویر، استایل‌ها و متادیتاهای صفحات را استخراج کرده و در قالب یک فایل ZIP ذخیره کند.

## ویژگی‌ها

- **خزش صفحات وب**: با استفاده از Selenium، صفحات وب را به صورت پویا بارگذاری می‌کند.
- **استخراج متن**: متن اصلی صفحات را استخراج و ذخیره می‌کند.
- **ذخیره‌سازی تصاویر**: تصاویر موجود در صفحات را دانلود و ذخیره می‌کند.
- **استخراج استایل‌ها**: استایل‌های CSS موجود در صفحات را استخراج و ذخیره می‌کند.
- **ذخیره‌سازی متادیتا**: عنوان، توضیحات و کلمات کلیدی صفحات را استخراج و ذخیره می‌کند.
- **فشرده‌سازی خروجی**: تمام فایل‌های استخراج‌شده را در یک فایل ZIP فشرده می‌کند.

## نیازمندی‌ها

برای اجرای این اسکرپر، نیاز به نصب کتابخانه‌های زیر دارید:

- **Python 3.7 یا بالاتر**
- **Selenium**
- **BeautifulSoup4**
- **Requests**
- **tqdm**
- **Zipfile**

برای نصب کتابخانه‌ها، دستور زیر را اجرا کنید:

```bash
pip install selenium beautifulsoup4 requests tqdm
```

## نحوه استفاده

1. **تنظیم URL پایه**: در فایل `scraper.py`، متغیر `BASE_URL` را به آدرس وب‌سایت مورد نظر خود تغییر دهید.

   ```python
   BASE_URL = "https://example.com"
   ```

2. **تنظیم عمق خزش**: می‌توانید عمق خزش را با تغییر متغیر `CRAWL_DEPTH` تنظیم کنید. به عنوان مثال، برای خزش تا عمق ۲:

   ```python
   CRAWL_DEPTH = 2
   ```

3. **اجرای اسکرپر**: اسکرپر را با اجرای دستور زیر اجرا کنید:

   ```bash
   python scraper.py
   ```

4. **خروجی**: پس از اتمام فرآیند، یک فایل ZIP با نام `scraped_site.zip` ایجاد می‌شود که شامل تمام فایل‌های استخراج‌شده است.

## ساختار فایل‌های خروجی

- **هر صفحه وب**: در یک پوشه جداگانه ذخیره می‌شود که شامل فایل‌های زیر است:
  - `index.html`: کد HTML صفحه.
  - `content.txt`: متن استخراج‌شده از صفحه.
  - `global.css`: استایل‌های CSS استخراج‌شده.
  - `metadata.txt`: متادیتاهای صفحه (عنوان، توضیحات، کلمات کلیدی).
  - تصاویر: تمام تصاویر موجود در صفحه.

- **فایل ZIP**: تمام پوشه‌ها و فایل‌ها در یک فایل ZIP فشرده می‌شوند.

## نکات مهم

- **Selenium WebDriver**: این اسکرپر از **Chrome WebDriver** استفاده می‌کند. اطمینان حاصل کنید که ChromeDriver نصب شده و در مسیر سیستم قرار دارد.
- **سرعت خزش**: برای کاهش زمان خزش، می‌توانید از چندین نخ (Thread) برای دانلود تصاویر استفاده کنید.
- **محدودیت‌ها**: این اسکرپر برای صفحات استاتیک و نیمه‌پویا مناسب است. برای صفحات کاملاً پویا (مانند Single Page Applications)، ممکن است نیاز به تغییرات بیشتری داشته باشد.

## مثال

فرض کنید می‌خواهید محتوای وب‌سایت `https://example.com` را استخراج کنید. مراحل زیر را دنبال کنید:

1. `BASE_URL` را به `https://example.com` تغییر دهید.
2. اسکرپر را اجرا کنید:

   ```bash
   python scraper.py
   ```

3. پس از اتمام فرآیند، فایل `scraped_site.zip` را بررسی کنید.

## توسعه و مشارکت

اگر می‌خواهید در توسعه این پروژه مشارکت کنید، مراحل زیر را دنبال کنید:

1. ریپازیتوری را Fork کنید.
2. تغییرات خود را اعمال کنید.
3. یک Pull Request ارسال کنید.

## لایسنس

این پروژه تحت لایسنس [MIT](LICENSE) منتشر شده است.

---

**نویسنده**: [سجاداکبری]  
**ایمیل**: [Sajjadakbari]  
**تاریخ انتشار**: [1403/12/19]

### توضیحات:

- این فایل `README.md` شامل تمام اطلاعات لازم برای استفاده از اسکرپر است.
- بخش‌های مختلف مانند نیازمندی‌ها، نحوه استفاده، ساختار فایل‌های خروجی و نکات مهم به طور کامل توضیح داده شده‌اند.
- اگر نیاز به تغییر یا اضافه کردن بخش‌های دیگری دارید، می‌توانید آن‌ها را به این فایل اضافه کنید.


## سوالات متداول (FAQ)

### سوال: آیا امکان ترجمه محتوا به زبان‌های دیگر وجود دارد؟

**پاسخ:**  
بله، شما می‌توانید محتوا را به زبان‌های دیگر ترجمه کنید. به جای استفاده از API ترجمه، می‌توانید از کتابخانه‌هایی مانند **`translate`** در پایتون استفاده کنید. به مثال زیر توجه کنید:

```python
from translate import Translator

translator = Translator(to_lang="fa")
translation = translator.translate("Hello, World!")
print(translation)  # خروجی: سلام دنیا!










اگر سوالی داشتید، خوشحال می‌شوم کمک کنم! 😊


