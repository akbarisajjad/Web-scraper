# Web-scraper-plus
این نسخه تقریباً نهایی است و تمام قابلیت‌هایی که گفتید را پیاده‌سازی می‌کند. اما اگر بخواهید ویژگی‌های پیشرفته‌تری اضافه شود، می‌توانیم بهینه‌سازی‌های نهایی را انجام دهیم.

📌 مواردی که می‌توان بهبود داد:

1. بهینه‌سازی دریافت تصاویر و فایل‌ها:

اگر تصاویر زیادی باشند، می‌توان چند پردازشی (multithreading) اضافه کرد تا دانلود سریع‌تر شود.



2. فشرده‌سازی خروجی:

قابلیت ذخیره تمام داده‌های صفحه در یک فایل ZIP.



3. ذخیره اطلاعات متا (عنوان، توضیحات، کلمات کلیدی)

metadata.txt شامل:

<title>

<meta name="description">

<meta name="keywords">




4. امکان استخراج فقط از یک بخش خاص (مثلاً فقط مقاله، فقط نظرات و...):

قابلیت انتخاب بخش‌های خاص (مثلاً فقط <article> را استخراج کند).



5. پشتیبانی از سایت‌های داینامیک (React, Vue, AJAX)

در حال حاضر فقط HTML استاتیک را می‌گیرد، اما می‌توان از Selenium برای سایت‌های داینامیک استفاده کرد.



6. امکان تعیین عمق خزش (Crawl Depth)

مشخص کنید تا چند سطح از لینک‌های داخلی بررسی شود.





---

📢 آیا این نسخه نیاز شما را برطرف می‌کند یا می‌خواهید این قابلیت‌ها را هم اضافه کنم؟


خیلی هم عالی! من نسخه نهایی و فوق‌پیشرفته این وب‌اسکرپر را برایت آماده می‌کنم که همه قابلیت‌های موردنظر را داشته باشد. ویژگی‌های جدید شامل:

✅ چندپردازشی برای دانلود سریع‌تر تصاویر و فایل‌ها
✅ فشرده‌سازی تمام داده‌های صفحه در یک فایل ZIP
✅ استخراج اطلاعات متا (title, description, keywords)
✅ قابلیت استخراج فقط از یک بخش خاص (مثلاً فقط مقاله، نظرات و...)
✅ پشتیبانی از سایت‌های داینامیک (React, Vue, AJAX) با Selenium
✅ امکان تنظیم عمق خزش (Crawl Depth)

کد را در حال آماده‌سازی دارم و به‌زودی نسخه کامل را ارسال می‌کنم! 🚀


برای اجرای این وب اسکرپر، باید کتابخانه‌های زیر را نصب کنید:

📌 کتابخانه‌های مورد نیاز:
برای نمایش متن `console.log("Hello, World!");` درون خط، از بک‌تیک استفاده کنید.
```
pip install requests beautifulsoup4 selenium tqdm
```
📌 توضیحات کتابخانه‌ها:
```
✅ requests → برای دانلود فایل‌ها و تصاویر از وب‌سایت‌ها
✅ beautifulsoup4 → برای پردازش و استخراج اطلاعات از HTML
✅ selenium → برای دریافت صفحات داینامیک (React, Vue, AJAX)
✅ tqdm → برای نمایش نوار پیشرفت دانلود
```

---

📌 نصب در ویندوز یا لینوکس
```
pip install -r requirements.txt
```
یا مستقیماً:
```
pip install requests beautifulsoup4 selenium tqdm
```
📌 نصب در macOS (در صورت نیاز به درایور کروم)
```
brew install chromedriver
```
اگر Chromedriver نصب نیست، می‌توان از WebDriver Manager استفاده کرد:
```
pip install webdriver-manager
```
و در کد این خط را اضافه کنید:
```
from webdriver_manager.chrome import ChromeDriverManager
driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)
```
🔍 معرفی وب‌اسکرپر پیشرفته

این وب‌اسکرپر پیشرفته می‌تواند تمام محتوای یک سایت را بدون به‌هم‌ریختگی ساختار استخراج کند. از سایت‌های استاتیک گرفته تا صفحات داینامیک (React, Vue, AJAX)، همه را دریافت می‌کند و در فولدرهای منظم ذخیره می‌کند.

📌 قابلیت‌های کلیدی

1️⃣ دانلود کامل صفحات سایت

✅ دریافت کامل HTML صفحه و ذخیره به همان شکل اصلی در فایل index.html
✅ استخراج و ذخیره متن صفحه بدون کدهای اضافی در فایل content.txt
✅ ذخیره متادیتا (عنوان، توضیحات و کلمات کلیدی) در metadata.txt


---

2️⃣ استخراج و ذخیره استایل‌ها (CSS) به‌صورت جداگانه

✅ فایل global.css شامل تمام استایل‌های صفحه (تگ <style>)
✅ دانلود فایل‌های CSS خارجی (.css) و ذخیره در فولدر styles/
✅ استایل هر بخش مانند هدر، فوتر، محتوا و... به‌صورت جداگانه حفظ می‌شود


---

3️⃣ دانلود و ذخیره تصاویر و فایل‌ها

✅ تمام تصاویر (.jpg, .png, .svg, .gif) به فولدر images/ دانلود می‌شود
✅ پشتیبانی از فایل‌های ویدیویی (.mp4, .webm) و ذخیره در videos/
✅ چندپردازشی (Multi-threading) برای افزایش سرعت دانلود تصاویر


---

4️⃣ پشتیبانی از سایت‌های داینامیک (React, Vue, AJAX)

✅ استفاده از Selenium برای دریافت صفحات JavaScript-based
✅ استخراج محتوای سایت‌هایی که محتوا را به‌صورت AJAX لود می‌کنند
✅ اجرای خودکار مرورگر کروم بدون نیاز به باز کردن آن (Headless Mode)


---

5️⃣ استخراج از بخش‌های خاص صفحه (قابلیت انتخابی)

✅ می‌توانید مشخص کنید که فقط بخش‌های خاصی از سایت را استخراج کند
✅ مثال: فقط <article> (مقاله‌ها)، <comments> (نظرات کاربران) یا <main>


---

6️⃣ تنظیم عمق خزش (Crawl Depth)

✅ امکان مشخص کردن تا چند سطح از لینک‌های داخلی پردازش شود
✅ مثال:

CRAWL_DEPTH = 0 → فقط صفحه اصلی

CRAWL_DEPTH = 1 → صفحه اصلی + لینک‌های داخلی آن

CRAWL_DEPTH = 2 → صفحه اصلی + لینک‌های داخلی + لینک‌های داخلی آن‌ها



---

7️⃣ فشرده‌سازی خروجی (ZIP)

✅ تمام فایل‌های استخراج‌شده در یک فایل ZIP ذخیره می‌شوند
✅ امکان ارسال یا انتقال سریع به سیستم‌های دیگر


---

📂 ساختار خروجی
```
📂 scraped_site/ (هر صفحه در یک فولدر مجزا ذخیره می‌شود)

scraped_site/
│── index.html       (کد کامل HTML صفحه)
│── content.txt      (متن خالص بدون کدهای اضافی)
│── metadata.txt     (عنوان، توضیحات و کلمات کلیدی)
│── global.css       (استایل‌های داخلی صفحه)
│── images/
│   ├── image1.jpg
│   ├── image2.png
│── styles/
│   ├── main.css
│   ├── theme.css
│── scripts/
│   ├── script.js
│── videos/
│   ├── video.mp4
scraped_site.zip     (کل داده‌ها در یک فایل ZIP)
```

---

🔧 کتابخانه‌های موردنیاز و نحوه اجرا

📌 نصب کتابخانه‌های موردنیاز
```
pip install requests beautifulsoup4 selenium tqdm webdriver-manager
```
📌 اجرای اسکرپر
```
python scraper.py
```
(آدرس سایت را در متغیر BASE_URL تنظیم کنید)


---

⚡ چرا این وب‌اسکرپر عالی است؟

✅ پشتیبانی از سایت‌های معمولی + سایت‌های داینامیک
✅ دانلود سریع تصاویر و فایل‌ها با چندپردازشی
✅ ساختار خروجی کاملاً منظم و دسته‌بندی‌شده
✅ قابلیت فشرده‌سازی داده‌ها در فایل ZIP
✅ استخراج استایل‌ها، متن‌ها و متادیتا به‌صورت جداگانه
✅ بهینه‌شده برای انواع سایت‌ها و مقیاس‌های مختلف


---

💡#قابلیت های دیگری که در نسخه بعدی به این اسکرپر اضافه میکنم موارد زیر است👇
```
#

```
🚀 ##قابلیت‌های پیشرفته‌ای که می‌توان به اسکرپر اضافه کرد

اگر بخواهید اسکرپر را به یک ابزار فوق‌العاده حرفه‌ای تبدیل کنید، می‌توان ویژگی‌های زیر را اضافه کرد:


---

1️⃣ ##پشتیبانی از ورود به سایت‌ها (Login)

✅ استخراج محتوا از سایت‌هایی که نیاز به ورود دارند
✅ استفاده از نام کاربری و رمز عبور برای ورود خودکار
✅ پشتیبانی از کوکی‌ها و توکن‌های احراز هویت (JWT, Session)

🔹 کاربرد: دانلود مقالات از سایت‌های عضویت ویژه مثل Medium, ResearchGate, Coursera و...


---

2️⃣ #دریافت خودکار تمام صفحات یک سایت (Site Crawler)

✅ امکان خزش (Crawling) کل سایت
✅ ذخیره تمام صفحات و لینک‌های داخلی در یک ساختار منظم
✅ امکان تعیین محدودیت عمق خزش (مثلاً فقط ۲ سطح از لینک‌های داخلی)

🔹 کاربرد: استخراج کل محتوای سایت‌ها برای تحلیل محتوا، سئو و آرشیو کردن


---

3️⃣ شبیه‌سازی رفتار کاربر (Anti-Bot Bypass)

✅ تغییر User-Agent برای دور زدن ربات‌گیرها
✅ پشتیبانی از پروکسی و VPN برای تغییر آی‌پی
✅ قابلیت استفاده از کپچابریکر (Captcha Solver) برای عبور از کپچا

🔹 کاربرد: دانلود محتوا از سایت‌هایی که سیستم ضدربات دارند مثل Cloudflare، Akamai و...


---

4️⃣ استخراج اطلاعات ساختاریافته (Schema & JSON-LD)

✅ شناسایی اطلاعات ساختاریافته (Structured Data)
✅ استخراج کدهای JSON-LD و میکرودیتا از صفحه
✅ ذخیره داده‌ها در فایل JSON جداگانه

🔹 کاربرد: برای تحلیل سئو، استخراج اطلاعات محصولات، اخبار و مقالات از سایت‌ها


---

5️⃣ پردازش و خلاصه‌سازی محتوای متنی

✅ خلاصه‌سازی خودکار متن با هوش مصنوعی (NLTK, Transformers)
✅ امکان تحلیل احساسات (Sentiment Analysis)
✅ تبدیل متن به فرمت Markdown یا PDF

🔹 کاربرد: برای ایجاد گزارش‌های تحلیلی از سایت‌ها و استخراج مهم‌ترین نکات محتوا


---

6️⃣ دانلود فایل‌های پیوست (PDF, DOCX, ZIP)

✅ شناسایی و دانلود تمام فایل‌های PDF، Word، اکسل و ZIP
✅ ذخیره متن داخلی فایل‌های PDF و Word در text.txt
✅ تبدیل فایل‌های PDF به قالب متنی قابل ویرایش

🔹 کاربرد: دانلود گزارش‌ها، مقالات دانشگاهی و فایل‌های منابع


---

7️⃣ ذخیره‌سازی داده‌ها در دیتابیس

✅ امکان ذخیره داده‌ها در SQLite, MySQL یا PostgreSQL
✅ دسته‌بندی اطلاعات به‌صورت ساختار‌یافته و جست‌وجوپذیر
✅ ارائه API برای دسترسی به داده‌ها از طریق برنامه‌های دیگر

🔹 کاربرد: ایجاد آرشیو دائمی از اطلاعات سایت‌ها و امکان جست‌وجوی حرفه‌ای


---

8️⃣ پشتیبانی از سایت‌های چندزبانه

✅ شناسایی زبان محتوای صفحه
✅ امکان ترجمه خودکار محتوا به زبان‌های دیگر با Google Translate API
✅ ذخیره نسخه اصلی و ترجمه‌شده در فایل‌های جداگانه

🔹 کاربرد: اسکرپ کردن مقالات چندزبانه و سایت‌های بین‌المللی


---

9️⃣ دانلود کامنت‌ها و نظرات کاربران

✅ استخراج نظرات کاربران از وب‌سایت‌ها و شبکه‌های اجتماعی
✅ ذخیره نظرات به‌صورت متن، JSON یا CSV
✅ امکان تشخیص و فیلتر کردن نظرات اسپم

🔹 کاربرد: تحلیل نظرات کاربران در سایت‌های فروشگاهی، فروم‌ها و رسانه‌ها


---

🔗 آیا این قابلیت‌ها را می‌خواهید اضافه کنیم؟ یا اولویت‌های خاصی دارید؟ 🚀

🚀 قابلیت‌های پیشرفته‌ای که می‌توان به اسکرپر اضافه کرد


✅ تبدیل متن به فرمت Markdown یا PDF

🔹 کاربرد: برای ایجاد گزارش‌های تحلیلی از سایت‌ها و استخراج مهم‌ترین نکات محتوا


---

6️⃣ دانلود فایل‌های پیوست (PDF, DOCX, ZIP)

✅ شناسایی و دانلود تمام فایل‌های PDF، Word، اکسل و ZIP
✅ ذخیره متن داخلی فایل‌های PDF و Word در text.txt
✅ تبدیل فایل‌های PDF به قالب متنی قابل ویرایش

🔹 کاربرد: دانلود گزارش‌ها، مقالات دانشگاهی و فایل‌های منابع


---

7️⃣ ذخیره‌سازی داده‌ها در دیتابیس

✅ امکان ذخیره داده‌ها در SQLite, MySQL یا PostgreSQL
✅ دسته‌بندی اطلاعات به‌صورت ساختار‌یافته و جست‌وجوپذیر
✅ ارائه API برای دسترسی به داده‌ها از طریق برنامه‌های دیگر

🔹 کاربرد: ایجاد آرشیو دائمی از اطلاعات سایت‌ها و امکان جست‌وجوی حرفه‌ای


---

8️⃣ پشتیبانی از سایت‌های چندزبانه

✅ شناسایی زبان محتوای صفحه
✅ امکان ترجمه خودکار محتوا به زبان‌های دیگر با Google Translate API
✅ ذخیره نسخه اصلی و ترجمه‌شده در فایل‌های جداگانه

🔹 کاربرد: اسکرپ کردن مقالات چندزبانه و سایت‌های بین‌المللی


---

9️⃣ دانلود کامنت‌ها و نظرات کاربران

✅ استخراج نظرات کاربران از وب‌سایت‌ها و شبکه‌های اجتماعی
✅ ذخیره نظرات به‌صورت متن، JSON یا CSV
✅ امکان تشخیص و فیلتر کردن نظرات اسپم

🔹 کاربرد: تحلیل نظرات کاربران در سایت‌های فروشگاهی، فروم‌ها و رسانه‌ها


---

